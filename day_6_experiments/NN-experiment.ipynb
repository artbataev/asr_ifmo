{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_files.pickle\", \"rb\") as f:\n",
    "    files_train = pickle.load(f)\n",
    "with open(\"val_files.pickle\", \"rb\") as f:\n",
    "    files_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = []\n",
    "phonemes_train = []\n",
    "mfcc_train = []\n",
    "video_train = []\n",
    "for file in files_train:\n",
    "    labels_train.append(np.load(file)[\"labels\"])\n",
    "    phonemes_train.append(np.load(file)[\"phonemes\"])\n",
    "    mfcc_train.append(np.load(file)[\"mfcc\"])\n",
    "    video_train.append(np.load(file)[\"video\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:07<00:00, 133.52it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_val = []\n",
    "phonemes_val = []\n",
    "mfcc_val = []\n",
    "video_val = []\n",
    "for file in tqdm.tqdm(files_val):\n",
    "    labels_val.append(np.load(file)[\"labels\"])\n",
    "    phonemes_val.append(np.load(file)[\"phonemes\"])\n",
    "    mfcc_val.append(np.load(file)[\"mfcc\"])\n",
    "    video_val.append(np.load(file)[\"video\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def conv_layer(prev_layer, filter_width, stride, input_channels, output_channels,\n",
    "#                layer_id, use_relu=True, use_batchnorn=False, is_training=None):\n",
    "#     with tf.variable_scope('conv_{}'.format(layer_id)):\n",
    "#         convolution_out = tf.layers.conv1d(prev_layer, output_channels, \n",
    "#                                            kernel_size=filter_width, strides=stride, activation=None)\n",
    "#         if use_batchnorn:\n",
    "#             if is_training is None:\n",
    "#                 raise Exception(\"is_training placeholder required\")\n",
    "#             convolution_out = tf.layers.batch_normalization(convolution_out, training=is_training)\n",
    "#         if use_relu:\n",
    "#             convolution_out = tf.nn.relu(convolution_out, name='activation')\n",
    "#         return convolution_out, output_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import xavier_initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, filter_width, stride, input_channels, output_channels,\n",
    "               layer_id, use_relu=True, use_leaky_relu=False, use_batchnorn=False, is_training=None):\n",
    "    with tf.variable_scope('conv_{}'.format(layer_id)):\n",
    "        filters = tf.get_variable('filters', shape=[filter_width, input_channels, output_channels],\n",
    "                                  dtype=tf.float32, initializer=xavier_initializer())\n",
    "        bias = tf.Variable(tf.constant(0.0, shape=[output_channels]), name='bias')\n",
    "        convolution_out = tf.nn.conv1d(prev_layer, filters, stride, 'SAME', use_cudnn_on_gpu=True, name='convolution')\n",
    "        convolution_out = tf.nn.bias_add(convolution_out, bias)\n",
    "        if use_batchnorn:\n",
    "            if is_training is None:\n",
    "                raise Exception(\"is_training placeholder required\")\n",
    "            convolution_out = tf.layers.batch_normalization(convolution_out, training=is_training)\n",
    "        if use_relu:\n",
    "            if not use_leaky_relu:\n",
    "                activations = tf.nn.relu(convolution_out, name='activation')\n",
    "            else:\n",
    "                activations = tflearn.activations.leaky_relu(convolution_out, alpha=0.01, name=\"activation\")\n",
    "            return activations, output_channels\n",
    "        else:\n",
    "            return convolution_out, output_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-82-e73c856cbca2>, line 213)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-82-e73c856cbca2>\"\u001b[0;36m, line \u001b[0;32m213\u001b[0m\n\u001b[0;31m    self.save_weights(sess)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class SpeechDNNModel:\n",
    "    def __init__(self, num_features=13, num_symbols=23 + 1, optimizer=\"adam\", use_batchnorm=True):\n",
    "        tf.reset_default_graph()\n",
    "        self.num_features = num_features\n",
    "        self.num_symbols = num_symbols\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.min_dev_loss = np.float('inf')\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.optimizer_type = optimizer\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self._build_graph()\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.features_placeholder = tf.placeholder(tf.float32, [None, None, self.num_features], name=\"features\")\n",
    "        self.features_len_placeholder = tf.placeholder(tf.int32, [None], name=\"features_len\")\n",
    "        self.target_placeholder = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "        self.target_len_placeholder = tf.placeholder(tf.int32, [None], name=\"targets_len\")\n",
    "\n",
    "        self.learning_rate = tf.placeholder_with_default(1e-4, [], name=\"learning_rate\")\n",
    "        self.is_training = tf.placeholder_with_default(False, [], name=\"is_training\")\n",
    "\n",
    "        # transform target to sparse\n",
    "        target_ind = tf.where(tf.not_equal(tf.sequence_mask(self.target_len_placeholder), False))\n",
    "        target_val = tf.gather_nd(self.target_placeholder, target_ind)\n",
    "        self.target_sparse = tf.SparseTensor(target_ind, target_val,\n",
    "                                             tf.cast(tf.shape(self.target_placeholder), dtype=tf.int64))\n",
    "\n",
    "    def _inference(self):\n",
    "        # define architecture\n",
    "        outputs, channels = conv_layer(self.features_placeholder,\n",
    "                                       filter_width=16, stride=1,\n",
    "                                       input_channels=self.num_features, output_channels=256,\n",
    "                                       layer_id=1,\n",
    "                                       use_batchnorn=self.use_batchnorm,\n",
    "                                       is_training=self.is_training)\n",
    "\n",
    "        # 7 layers without striding of output size [batch_size, max_time / 2, 250]\n",
    "        for layer_id in range(2):\n",
    "            outputs, channels = conv_layer(outputs, 4, 1, channels, channels, layer_id + 2,\n",
    "                                           use_batchnorn=self.use_batchnorm,\n",
    "                                           is_training=self.is_training)\n",
    "\n",
    "        # 1 layer with high kernel width and output size [batch_size, max_time / 2, 2000]\n",
    "        outputs, channels = conv_layer(outputs, 8, 1, channels, channels * 4, layer_id=10,\n",
    "                                       use_batchnorn=self.use_batchnorm,\n",
    "                                       is_training=self.is_training)\n",
    "\n",
    "        # 1 fully connected layer of output size [batch_size, max_time / 2, 2000]\n",
    "        outputs, channels = conv_layer(outputs, 1, 1, channels, channels, layer_id=11,\n",
    "                                       use_batchnorn=self.use_batchnorm,\n",
    "                                       is_training=self.is_training)\n",
    "\n",
    "        # 1 fully connected layer of output size [batch_size, max_time / 2, num_classes]\n",
    "        outputs, channels = conv_layer(outputs, 1, 1, channels, self.num_symbols, 12, use_relu=False,\n",
    "                                       use_batchnorn=False)\n",
    "\n",
    "        logits_batch_major = outputs\n",
    "\n",
    "        self.logits = tf.transpose(logits_batch_major, [1, 0, 2])\n",
    "\n",
    "    def _create_loss(self):\n",
    "        # predictions = tf.nn.ctc_beam_search_decoder(logits_time_major, mfcc_len_placeholder // 2, beam_width=20)\n",
    "        self.predictions, _ = tf.nn.ctc_greedy_decoder(self.logits,\n",
    "                                                       self.features_len_placeholder)  # predictions - sparse tensor!\n",
    "\n",
    "        self.LER = tf.edit_distance(self.predictions[0], tf.cast(self.target_sparse, tf.int64))\n",
    "        with tf.name_scope('training'):\n",
    "#             cost = tf.nn.ctc_loss(self.target_sparse, self.logits, self.features_len_placeholder, time_major=True)\n",
    "            cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.target_placeholder)\n",
    "            corrected_cost = tf.minimum(cost, 400.0)\n",
    "            self.loss = tf.reduce_mean(corrected_cost, name='average_loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        if self.optimizer_type == \"adam\":\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        elif self.optimizer_type == \"yellowfin\":\n",
    "            raise ValueError(\"not now\")\n",
    "#             self.optimizer = YFOptimizer(self.yellowfin_lr)\n",
    "#             print(\"using YellowFin with lr {:f}\".format(self.yellowfin_lr))\n",
    "        else:\n",
    "            raise ValueError(\"incorrect optimizer\")\n",
    "\n",
    "        gvs = self.optimizer.compute_gradients(self.loss)\n",
    "        gradients, trainables = zip(*gvs)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients, 5.0, name='clip_gradients')\n",
    "        # batch normalization in tensorflow requires this extra dependency\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, trainables),\n",
    "                                                             global_step=self.global_step)\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._inference()\n",
    "        self._create_loss()\n",
    "        self._network_vars = tf.global_variables()  # without optimizer\n",
    "        self._create_optimizer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        # self._create_summary()\n",
    "\n",
    "    def save_weights(self, sess):\n",
    "        self.saver.save(sess, '/home/artbataev/Documents/checkpoints/dnn_mfcc/adam_checkpoints_best/ckpt',\n",
    "                        self.step)\n",
    "\n",
    "    def restore_weights(self, sess, only_network=False):\n",
    "        ckpt = tf.train.get_checkpoint_state(\n",
    "            '/home/artbataev/Documents/checkpoints/dnn_mfcc/adam_checkpoints_best/')\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            if only_network:\n",
    "                saver = tf.train.Saver(self._network_vars)\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def init_op(self, sess, restore=True, restore_only_network=False):\n",
    "        # sess.run(tf.global_variables_initializer())\n",
    "        if restore:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.restore_weights(sess, only_network=restore_only_network)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def test_net(self, batch_generator, sess, verbose=False):\n",
    "        last = False\n",
    "        full_len = 0\n",
    "        loss = 0.0\n",
    "        LER = 0.0\n",
    "        WER = 0.0\n",
    "        while not last:\n",
    "            texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "            feed_dict = {\n",
    "                self.features_placeholder: mfcc,\n",
    "                self.features_len_placeholder: mfcc_len,\n",
    "                self.target_placeholder: texts,\n",
    "                self.target_len_placeholder: texts_len\n",
    "            }\n",
    "\n",
    "            if verbose:\n",
    "                current_loss, predicted, current_LER = sess.run([self.loss, self.predictions, self.LER],\n",
    "                                                                feed_dict=feed_dict)\n",
    "            else:\n",
    "                current_loss, current_LER = sess.run([self.loss, self.LER], feed_dict=feed_dict)\n",
    "\n",
    "            loss += current_loss * len(texts)\n",
    "            LER += np.sum(current_LER)\n",
    "            full_len += len(texts)\n",
    "            if verbose:\n",
    "                orig_sentence = batch_generator.get_encoder().decode(texts[0][:texts_len[0]])\n",
    "                pred_sentences_compact = batch_generator.get_encoder().decode_texts_sparse(predicted[0].indices,\n",
    "                                                                                           predicted[0].values,\n",
    "                                                                                           predicted[0].dense_shape)\n",
    "                distance = current_LER[0]\n",
    "                print(\"Average loss: {:.5f}, Average LER: {:.3f}% LER: {:.2f}%\".format(current_loss,\n",
    "                                                                                       np.mean(current_LER) * 100,\n",
    "                                                                                       distance * 100))\n",
    "                print(\"Original: \", orig_sentence)\n",
    "                print(\"--Predicted: \", pred_sentences_compact[0])\n",
    "                print(\"-\" * 50)\n",
    "                for i, text in enumerate(texts):\n",
    "                    orig = batch_generator.get_encoder().decode(text[:texts_len[i]])\n",
    "                    WER += editdistance.eval(orig.split(), pred_sentences_compact[i].split()) / len(orig.split())\n",
    "        LER /= full_len\n",
    "        WER /= full_len\n",
    "        loss /= full_len\n",
    "        if verbose:\n",
    "            print(\"Average test loss: {:.5f}\".format(loss))\n",
    "            print(\"Average test LER: {:.2f}%\".format(LER * 100))\n",
    "            print(\"Average test WER: {:.2f}%\".format(WER * 100))\n",
    "        return loss, LER\n",
    "\n",
    "    def run_step(self, batch_generator, sess, lr=1e-4):\n",
    "        texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "        feed_dict = {\n",
    "            self.features_placeholder: mfcc,\n",
    "            self.features_len_placeholder: mfcc_len,\n",
    "            self.target_placeholder: texts,\n",
    "            self.target_len_placeholder: texts_len,\n",
    "            self.is_training: True,\n",
    "            self.learning_rate: lr,\n",
    "        }\n",
    "        current_loss, current_LER, _ = sess.run([self.loss, self.LER, self.train_step], feed_dict=feed_dict)\n",
    "        self.step += 1\n",
    "        if last:\n",
    "            self.epoch += 1\n",
    "            print(\"Epoch {}, end of dataset\".format(self.epoch))\n",
    "        return current_loss, current_LER\n",
    "\n",
    "    def fit(self, batch_generator, dev_generator, sess, steps=1, lr=1e-4):\n",
    "        try:\n",
    "            train_loss = 0.0\n",
    "            train_LER = 0.0\n",
    "            for _ in range(steps):\n",
    "                current_loss, current_LER = self.run_step(batch_generator, sess, lr)\n",
    "                train_loss += current_loss\n",
    "                train_LER += np.mean(current_LER)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss /= 10\n",
    "                    train_LER /= 10\n",
    "                    print(\"Epoch {} step {} average_loss: {:.5f} LER: {:.2f}%\".format(\n",
    "                        self.epoch, self.step, train_loss, train_LER * 100))\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "\n",
    "#                 if self.step % 100 == 0 and train_loss < 350:\n",
    "#                     epoch_test_loss, epoch_LER = self.test_net(dev_generator, sess)\n",
    "#                     improved_text = \"improved\" if epoch_test_loss < self.min_dev_loss else \"not improved\"\n",
    "#                     print(\"Step {} test_loss: {:.5f} LER: {:.2f}%\".format(self.step, epoch_test_loss,\n",
    "#                                                                           np.mean(epoch_LER) * 100), improved_text)\n",
    "#                     if epoch_test_loss < self.min_dev_loss:\n",
    "#                         self.min_dev_loss = epoch_test_loss\n",
    "                        self.save_weights(sess)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss = 0.0\n",
    "                    train_LER = 0.0\n",
    "\n",
    "                if np.isnan(current_loss) or np.isinf(current_loss) or (current_loss == 400.0 and self.step > 200):\n",
    "                    print(\"step\", self.step, current_loss)\n",
    "                    print(\"=\" * 20)\n",
    "                    print(\"Big loss, restoring weights\")\n",
    "                    print(\"=\" * 20)\n",
    "                    try:\n",
    "                        self.restore_weights(sess)\n",
    "                    except:\n",
    "                        print(\"can't restore, initialize\")\n",
    "                        self.init_op(sess, restore=False)\n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = SpeechDNNModel(optimizer=\"adam\", use_batchnorm=True)\n",
    "sess = tf.Session()\n",
    "dnn.init_op(sess, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        indices = np.random.choice(len(labels_train), size=batch_size, replace=False)\n",
    "        texts_len = np.array(list(map(lambda x: len(x), (phonemes_train[i] for i in indices))))\n",
    "        mfcc_len = np.array(list(map(lambda x: len(x), (mfcc_train[i] for i in indices))))\n",
    "        texts = np.zeros((batch_size, np.max(texts_len)), dtype=np.int)\n",
    "        mfcc = np.zeros((batch_size, np.max(mfcc_len), 13))\n",
    "        #         texts, texts_len, mfcc, mfcc_len, last\n",
    "        for i in range(batch_size):\n",
    "            texts[i, :len(phonemes_train[indices[i]])] = phonemes_train[indices[i]]\n",
    "            mfcc[i, :len(mfcc_train[indices[i]]), :] = mfcc_train[indices[i]]\n",
    "        yield texts, texts_len, mfcc, mfcc_len, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = train_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, texts_len, mfcc, mfcc_len, last = next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts, texts_len, mfcc, mfcc_len, last = next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Epoch 0 step 110 average_loss: 1.68494 LER: 99.72%\n",
      ".........Epoch 0 step 120 average_loss: 1.64274 LER: 99.72%\n",
      ".........Epoch 0 step 130 average_loss: 1.59395 LER: 99.71%\n",
      ".........Epoch 0 step 140 average_loss: 1.76933 LER: 99.71%\n",
      ".........Epoch 0 step 150 average_loss: 1.65256 LER: 99.71%\n",
      ".........Epoch 0 step 160 average_loss: 1.70048 LER: 99.72%\n",
      ".........Epoch 0 step 170 average_loss: 1.68945 LER: 99.72%\n",
      ".........Epoch 0 step 180 average_loss: 1.59162 LER: 99.72%\n",
      ".........Epoch 0 step 190 average_loss: 1.59568 LER: 99.72%\n",
      ".........Epoch 0 step 200 average_loss: 1.57573 LER: 99.71%\n"
     ]
    }
   ],
   "source": [
    "dnn.fit(t, t, sess, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml3gpu)",
   "language": "python",
   "name": "ml3gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
