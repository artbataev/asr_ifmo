{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_files.pickle\", \"rb\") as f:\n",
    "    files_train = pickle.load(f)\n",
    "with open(\"val_files.pickle\", \"rb\") as f:\n",
    "    files_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8433/8433 [00:32<00:00, 256.06it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_train = []\n",
    "phonemes_train = []\n",
    "mfcc_train = []\n",
    "fbanks_train = []\n",
    "video_features_train = []\n",
    "# video_train = []\n",
    "for file in tqdm.tqdm(files_train):\n",
    "    with np.load(file) as data:\n",
    "        cur_labels = data[\"labels\"]\n",
    "        cur_phonemes = data[\"phonemes\"]\n",
    "        cur_mfcc = data[\"mfcc\"]\n",
    "        cur_mfcc = (cur_mfcc - cur_mfcc.mean(axis=0))/cur_mfcc.std(axis=0)\n",
    "        cur_fbanks = data[\"fbanks\"]\n",
    "        cur_fbanks = (cur_fbanks - cur_fbanks.mean(axis=0)) / cur_fbanks.std(axis=0)\n",
    "    #     cur_video = np.load(file)[\"video\"]\n",
    "    labels_train.append(cur_labels)\n",
    "    phonemes_train.append(cur_phonemes)\n",
    "    mfcc_train.append(cur_mfcc)\n",
    "    fbanks_train.append(cur_fbanks)\n",
    "    \n",
    "    with np.load(os.path.join(\"../data/lip_reading/synchronized/video_features/train/\", os.path.basename(file))) as data2:\n",
    "        cur_video_features = data2[\"video_features\"]\n",
    "    video_features_train.append(cur_video_features)\n",
    "    #     video_train.append(cur_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:03<00:00, 301.63it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_val = []\n",
    "phonemes_val = []\n",
    "mfcc_val = []\n",
    "video_val = []\n",
    "video_features_val = []\n",
    "fbanks_val = []\n",
    "for file in tqdm.tqdm(files_val):\n",
    "    with np.load(file) as data:\n",
    "        cur_labels = data[\"labels\"]\n",
    "        cur_phonemes = data[\"phonemes\"]\n",
    "        cur_mfcc = data[\"mfcc\"]\n",
    "        cur_mfcc = (cur_mfcc - cur_mfcc.mean(axis=0))/cur_mfcc.std(axis=0)\n",
    "        cur_fbanks = data[\"fbanks\"]\n",
    "        cur_fbanks = (cur_fbanks - cur_fbanks.mean(axis=0)) / cur_fbanks.std(axis=0)\n",
    "    labels_val.append(cur_labels)\n",
    "    phonemes_val.append(cur_phonemes)\n",
    "    mfcc_val.append(cur_mfcc)\n",
    "    fbanks_val.append(cur_fbanks)\n",
    "    \n",
    "    with np.load(os.path.join(\"../data/lip_reading/synchronized/video_features/train/\", os.path.basename(file))) as data2:\n",
    "        cur_video_features = data2[\"video_features\"]\n",
    "    video_features_val.append(cur_video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, filter_width, num_filters, strides=1, use_relu=True, use_batchnorn=True, is_training=None):\n",
    "    convolution_out = tf.layers.conv1d(prev_layer, num_filters, filter_width, strides=strides, padding=\"same\", \n",
    "                                  activation=None)\n",
    "    if use_batchnorn:\n",
    "        if is_training is None:\n",
    "            raise Exception(\"is_training placeholder required\")\n",
    "        convolution_out = tf.layers.batch_normalization(convolution_out, training=is_training)\n",
    "    if use_relu:\n",
    "        convolution_out = tf.nn.relu(convolution_out)\n",
    "    return convolution_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcFullDNNModel:\n",
    "    def __init__(self, num_features=13, num_symbols=23+1, optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_mfcc\"):\n",
    "        tf.reset_default_graph()\n",
    "        self.num_features = num_features\n",
    "        self.num_symbols = num_symbols\n",
    "        self.checkpoints_folder = folder\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.min_dev_loss = np.float('inf')\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.optimizer_type = optimizer\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self._build_graph()\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.features_placeholder = tf.placeholder(tf.float32, [None, None, self.num_features + 512], name=\"features\")\n",
    "        self.features_len_placeholder = tf.placeholder(tf.int32, [None], name=\"features_len\")\n",
    "        self.target_placeholder = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "        self.target_len_placeholder = tf.placeholder(tf.int32, [None], name=\"targets_len\")\n",
    "\n",
    "        self.learning_rate = tf.placeholder_with_default(1e-4, [], name=\"learning_rate\")\n",
    "        self.is_training = tf.placeholder_with_default(False, [], name=\"is_training\")\n",
    "\n",
    "        # transform target to sparse\n",
    "        target_ind = tf.where(tf.not_equal(tf.sequence_mask(self.target_len_placeholder), False))\n",
    "        target_val = tf.gather_nd(self.target_placeholder, target_ind)\n",
    "        self.target_sparse = tf.SparseTensor(target_ind, target_val,\n",
    "                                             tf.cast(tf.shape(self.target_placeholder), dtype=tf.int64))\n",
    "\n",
    "    def _inference(self):\n",
    "        # define architecture\n",
    "        outputs = self.features_placeholder # [:, :, :self.num_features]\n",
    "        if self.use_batchnorm:\n",
    "            outputs = tf.layers.batch_normalization(outputs, training=self.is_training)\n",
    "        outputs = conv_layer(outputs, filter_width=48, strides=2, num_filters=256,\n",
    "                             use_batchnorn=self.use_batchnorm,\n",
    "                             is_training=self.is_training)\n",
    "\n",
    "        for layer_id in range(7):\n",
    "            outputs = conv_layer(outputs, filter_width=7, num_filters=256,\n",
    "                                 use_batchnorn=self.use_batchnorm,\n",
    "                                 is_training=self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, filter_width=32, strides=2, num_filters = 256 * 4,\n",
    "                    use_batchnorn = self.use_batchnorm,\n",
    "                    is_training = self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, filter_width=1, strides=2, num_filters=256, \n",
    "                use_batchnorn = self.use_batchnorm,\n",
    "                is_training = self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, 1, num_filters=self.num_symbols, use_relu=False,\n",
    "        use_batchnorn = False)\n",
    "\n",
    "        logits_batch_major = outputs\n",
    "\n",
    "        self.logits = logits_batch_major  # tf.transpose(logits_batch_major, [1, 0, 2])\n",
    "        self.logits_time_major = tf.transpose(logits_batch_major, [1, 0, 2])\n",
    "\n",
    "\n",
    "    def _create_loss(self):\n",
    "        self.predictions, _ = tf.nn.ctc_greedy_decoder(self.logits_time_major,\n",
    "                                                       self.features_len_placeholder // 8)  # predictions - sparse tensor!\n",
    "        self.predictions, _ = tf.nn.ctc_beam_search_decoder(self.logits_time_major,\n",
    "                                                       self.features_len_placeholder // 8, beam_width=200)\n",
    "        self.LER = tf.edit_distance(self.predictions[0], tf.cast(self.target_sparse, tf.int64))\n",
    "        with tf.name_scope('training'):\n",
    "            cost = tf.nn.ctc_loss(self.target_sparse, self.logits_time_major, self.features_len_placeholder // 8, time_major=True)\n",
    "            corrected_cost = tf.minimum(cost, 400.0)\n",
    "            self.loss = tf.reduce_mean(corrected_cost, name='average_loss')\n",
    "\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        if self.optimizer_type == \"adam\":\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        elif self.optimizer_type == \"yellowfin\":\n",
    "            raise ValueError(\"not now\")\n",
    "        else:\n",
    "            raise ValueError(\"incorrect optimizer\")\n",
    "\n",
    "        gvs = self.optimizer.compute_gradients(self.loss)\n",
    "        gradients, trainables = zip(*gvs)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients, 5.0, name='clip_gradients')\n",
    "        # batch normalization in tensorflow requires this extra dependency\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, trainables),\n",
    "                                                             global_step=self.global_step)\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._inference()\n",
    "        self._create_loss()\n",
    "        self._network_vars = tf.global_variables()  # without optimizer\n",
    "        self._create_optimizer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        # self._create_summary()\n",
    "\n",
    "\n",
    "    def save_weights(self, sess):\n",
    "        self.saver.save(sess, \n",
    "            '/home/artbataev/Documents/checkpoints/{}/adam_checkpoints_best/ckpt'.format(self.checkpoints_folder), \n",
    "            self.step)\n",
    "\n",
    "\n",
    "    def restore_weights(self, sess, only_network=False):\n",
    "        ckpt = tf.train.get_checkpoint_state(\n",
    "            '/home/artbataev/Documents/checkpoints/{}/adam_checkpoints_best/'.format(self.checkpoints_folder))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            if only_network:\n",
    "                saver = tf.train.Saver(self._network_vars)\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "    def init_op(self, sess, restore=True, restore_only_network=False):\n",
    "        if restore:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.restore_weights(sess, only_network=restore_only_network)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     def get_predictions_p(self, features, features_len):\n",
    "#         feed_dict = {\n",
    "#                 self.features_placeholder: features.reshape(1, features.shape[0], features.shape[1]),\n",
    "#                 self.features_len_placeholder: features_len}\n",
    "#         pred_p = sess.run(self.predictions_prob, feed_dict=feed_dict)\n",
    "#         return pred_p[0]\n",
    "    \n",
    "    def get_predictions(self, sess, features, features_len):\n",
    "        feed_dict = {\n",
    "                self.features_placeholder: features[np.newaxis,...],\n",
    "                self.features_len_placeholder: features_len,\n",
    "            }\n",
    "        pred = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "        return pred[0]\n",
    "    \n",
    "    def test_net(self, batch_generator, sess, verbose=False):\n",
    "        last = False\n",
    "        full_len = 0\n",
    "        loss = 0.0\n",
    "        LER = 0.0\n",
    "        while not last:\n",
    "            texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "            feed_dict = {\n",
    "                self.features_placeholder: mfcc,\n",
    "                self.features_len_placeholder: mfcc_len,\n",
    "                self.target_placeholder: texts,\n",
    "                self.target_len_placeholder: texts_len\n",
    "            }\n",
    "\n",
    "            current_loss, current_LER = sess.run([self.loss, self.LER], feed_dict=feed_dict)\n",
    "\n",
    "            loss += current_loss * len(texts)\n",
    "            LER += np.sum(current_LER)\n",
    "            full_len += len(texts)\n",
    "        \n",
    "        LER /= full_len\n",
    "        loss /= full_len\n",
    "        return loss, LER\n",
    "\n",
    "    def run_step(self, batch_generator, sess, lr=1e-4):\n",
    "        texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "        feed_dict = {\n",
    "            self.features_placeholder: mfcc,\n",
    "            self.features_len_placeholder: mfcc_len,\n",
    "            self.target_placeholder: texts,\n",
    "            self.target_len_placeholder: texts_len,\n",
    "            self.is_training: True,\n",
    "            self.learning_rate: lr,\n",
    "        }\n",
    "        current_loss, current_LER, _ = sess.run([self.loss, self.LER, self.train_step], feed_dict=feed_dict)\n",
    "        self.step += 1\n",
    "        if last:\n",
    "            self.epoch += 1\n",
    "            print(\"Epoch {}, end of dataset\".format(self.epoch))\n",
    "        return current_loss, current_LER\n",
    "\n",
    "    def fit(self, batch_generator, dev_generator, sess, steps=1, lr=1e-4):\n",
    "        try:\n",
    "            train_loss = 0.0\n",
    "            train_LER = 0.0\n",
    "            for _ in range(steps):\n",
    "                current_loss, current_LER = self.run_step(batch_generator, sess, lr)\n",
    "                train_loss += current_loss\n",
    "                train_LER += np.mean(current_LER)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss /= 10\n",
    "                    train_LER /= 10\n",
    "                    print(\"Epoch {} step {} average_loss: {:.5f} LER: {:.2f}%\".format(\n",
    "                        self.epoch, self.step, train_loss, train_LER * 100))\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "\n",
    "                if self.step % 20 == 0 and train_loss < 350:\n",
    "                    epoch_test_loss, epoch_LER = self.test_net(dev_generator, sess)\n",
    "                    improved_text = \"improved\" if epoch_test_loss < self.min_dev_loss else \"not improved\"\n",
    "                    print(\"Step {} test_loss: {:.5f} LER: {:.2f}%\".format(self.step, epoch_test_loss,\n",
    "                                                                          np.mean(epoch_LER) * 100), improved_text)\n",
    "                    if epoch_test_loss < self.min_dev_loss:\n",
    "                        self.min_dev_loss = epoch_test_loss\n",
    "                        self.save_weights(sess)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss = 0.0\n",
    "                    train_LER = 0.0\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        all_indices = np.random.choice(len(labels_train), len(labels_train), replace=False)\n",
    "        for j in range(0, len(labels_train), batch_size):\n",
    "            indices = all_indices[j: j + batch_size]\n",
    "            texts_len = np.array(list(map(lambda x: len(x), (labels_train[i] for i in indices))))\n",
    "            mfcc_len = np.array(list(map(lambda x: len(x), (mfcc_train[i] for i in indices))))\n",
    "            texts = np.zeros((texts_len.shape[0], np.max(texts_len)), dtype=np.int)\n",
    "            features = np.zeros((texts_len.shape[0], np.max(mfcc_len), 13 + 512))\n",
    "            #         texts, texts_len, mfcc, mfcc_len, last\n",
    "            for i in range(texts_len.shape[0]):\n",
    "                texts[i, :len(labels_train[indices[i]])] = labels_train[indices[i]]\n",
    "                features[i, :len(mfcc_train[indices[i]]), :] = np.hstack((mfcc_train[indices[i]], video_features_train[indices[i]]))\n",
    "            yield texts, texts_len, features, mfcc_len, j + batch_size >= len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        for j in range(0, len(labels_val), batch_size):\n",
    "            indices = np.arange(j, min(j + batch_size, len(labels_val)))\n",
    "            texts_len = np.array(list(map(lambda x: len(x), (labels_val[i] for i in indices))))\n",
    "            mfcc_len = np.array(list(map(lambda x: len(x), (mfcc_val[i] for i in indices))))\n",
    "            texts = np.zeros((texts_len.shape[0], np.max(texts_len)), dtype=np.int)\n",
    "            features = np.zeros((texts_len.shape[0], np.max(mfcc_len), 13 + 512))\n",
    "            #         texts, texts_len, mfcc, mfcc_len, last\n",
    "            for i in range(texts_len.shape[0]):\n",
    "                texts[i, :len(labels_val[indices[i]])] = labels_val[indices[i]]\n",
    "                features[i, :len(mfcc_val[indices[i]]), :] = np.hstack((mfcc_val[indices[i]], video_features_val[indices[i]]))\n",
    "            yield texts, texts_len, features, mfcc_len, j + batch_size >= len(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vgen = val_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tgen = train_generator()\n",
    "vgen = val_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts, texts_len, mfcc, mfcc_len, last = next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn = CtcFullDNNModel(optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_onlymfcc_ctc\")\n",
    "sess = tf.Session()\n",
    "dnn.init_op(sess, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Epoch 0 step 10 average_loss: 56.72946 LER: 131.70%\n",
      ".........Epoch 0 step 20 average_loss: 19.50456 LER: 88.12%\n",
      "Step 20 test_loss: 148.93799 LER: 315.33% improved\n",
      ".........Epoch 0 step 30 average_loss: 15.09070 LER: 72.07%\n",
      "..Epoch 1, end of dataset\n",
      ".......Epoch 1 step 40 average_loss: 8.57976 LER: 33.49%\n",
      "Step 40 test_loss: 153.89334 LER: 298.80% not improved\n",
      ".........Epoch 1 step 50 average_loss: 4.79055 LER: 18.00%\n",
      ".........Epoch 1 step 60 average_loss: 3.13647 LER: 11.86%\n",
      "Step 60 test_loss: 138.18154 LER: 280.67% improved\n",
      ".....Epoch 2, end of dataset\n",
      "....Epoch 2 step 70 average_loss: 2.26522 LER: 8.69%\n",
      ".........Epoch 2 step 80 average_loss: 1.81067 LER: 7.25%\n",
      "Step 80 test_loss: 72.70868 LER: 127.36% improved\n",
      ".........Epoch 2 step 90 average_loss: 1.66715 LER: 6.60%\n",
      "........Epoch 3, end of dataset\n",
      ".Epoch 3 step 100 average_loss: 1.51048 LER: 5.94%\n",
      "Step 100 test_loss: 36.18597 LER: 89.47% improved\n",
      ".........Epoch 3 step 110 average_loss: 1.07615 LER: 3.52%\n",
      ".........Epoch 3 step 120 average_loss: 0.92078 LER: 3.34%\n",
      "Step 120 test_loss: 24.53687 LER: 77.53% improved\n",
      ".........Epoch 3 step 130 average_loss: 0.89331 LER: 3.40%\n",
      ".Epoch 4, end of dataset\n",
      "........Epoch 4 step 140 average_loss: 0.65892 LER: 2.13%\n",
      "Step 140 test_loss: 16.31883 LER: 76.00% improved\n",
      ".........Epoch 4 step 150 average_loss: 0.49811 LER: 1.64%\n",
      ".........Epoch 4 step 160 average_loss: 0.60838 LER: 1.96%\n",
      "Step 160 test_loss: 11.63246 LER: 52.93% improved\n",
      "....Epoch 5, end of dataset\n",
      ".....Epoch 5 step 170 average_loss: 0.39865 LER: 1.17%\n",
      ".........Epoch 5 step 180 average_loss: 0.28376 LER: 0.73%\n",
      "Step 180 test_loss: 8.87917 LER: 37.88% improved\n",
      ".........Epoch 5 step 190 average_loss: 0.27877 LER: 0.77%\n",
      ".......Epoch 6, end of dataset\n",
      "..Epoch 6 step 200 average_loss: 0.28020 LER: 0.85%\n",
      "Step 200 test_loss: 5.84374 LER: 24.25% improved\n",
      ".........Epoch 6 step 210 average_loss: 0.16504 LER: 0.37%\n",
      ".........Epoch 6 step 220 average_loss: 0.18341 LER: 0.43%\n",
      "Step 220 test_loss: 4.79723 LER: 20.47% improved\n",
      ".........Epoch 6 step 230 average_loss: 0.17921 LER: 0.55%\n",
      "Epoch 7, end of dataset\n",
      ".........Epoch 7 step 240 average_loss: 0.13653 LER: 0.29%\n",
      "Step 240 test_loss: 2.79513 LER: 10.89% improved\n",
      ".........Epoch 7 step 250 average_loss: 0.12914 LER: 0.38%\n",
      ".........Epoch 7 step 260 average_loss: 0.11135 LER: 0.30%\n",
      "Step 260 test_loss: 1.95008 LER: 6.33% improved\n",
      "...Epoch 8, end of dataset\n",
      "......Epoch 8 step 270 average_loss: 0.08036 LER: 0.16%\n",
      ".........Epoch 8 step 280 average_loss: 0.07403 LER: 0.19%\n",
      "Step 280 test_loss: 2.06978 LER: 7.85% not improved\n",
      ".........Epoch 8 step 290 average_loss: 0.08816 LER: 0.22%\n",
      "......Epoch 9, end of dataset\n",
      "...Epoch 9 step 300 average_loss: 0.08058 LER: 0.25%\n",
      "Step 300 test_loss: 1.72059 LER: 6.56% improved\n",
      ".........Epoch 9 step 310 average_loss: 0.06372 LER: 0.11%\n",
      ".........Epoch 9 step 320 average_loss: 0.06116 LER: 0.12%\n",
      "Step 320 test_loss: 1.93852 LER: 6.94% not improved\n",
      ".........Epoch 10, end of dataset\n",
      "Epoch 10 step 330 average_loss: 0.08766 LER: 0.31%\n",
      ".........Epoch 10 step 340 average_loss: 0.04929 LER: 0.09%\n",
      "Step 340 test_loss: 1.51746 LER: 6.02% improved\n",
      ".........Epoch 10 step 350 average_loss: 0.04622 LER: 0.11%\n",
      ".........Epoch 10 step 360 average_loss: 0.05982 LER: 0.14%\n",
      "Step 360 test_loss: 1.64523 LER: 6.03% not improved\n",
      "..Epoch 11, end of dataset\n",
      ".......Epoch 11 step 370 average_loss: 0.05286 LER: 0.19%\n",
      ".........Epoch 11 step 380 average_loss: 0.04626 LER: 0.08%\n",
      "Step 380 test_loss: 1.42278 LER: 5.10% improved\n",
      ".........Epoch 11 step 390 average_loss: 0.04550 LER: 0.12%\n",
      ".....Epoch 12, end of dataset\n",
      "....Epoch 12 step 400 average_loss: 0.04922 LER: 0.16%\n",
      "Step 400 test_loss: 1.65697 LER: 5.52% not improved\n",
      ".........Epoch 12 step 410 average_loss: 0.04266 LER: 0.09%\n",
      ".........Epoch 12 step 420 average_loss: 0.04660 LER: 0.15%\n",
      "Step 420 test_loss: 1.74177 LER: 6.38% not improved\n",
      ".....Training Interrupted\n"
     ]
    }
   ],
   "source": [
    "dnn.fit(tgen, vgen, sess, steps=1000, lr=1e-4) # обучение модели только на mfcc - результаты хуже, чем на mfcc+video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/artbataev/Documents/checkpoints/dnn_onlymfcc_ctc/adam_checkpoints_best/ckpt-380\n"
     ]
    }
   ],
   "source": [
    "dnn.init_op(sess, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/artbataev/Documents/checkpoints/dnn_full_ctc_best/adam_checkpoints_best/ckpt-260\n"
     ]
    }
   ],
   "source": [
    "dnn = CtcFullDNNModel(optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_full_ctc_best\")\n",
    "sess = tf.Session()\n",
    "dnn.init_op(sess, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 s, sys: 864 ms, total: 6.51 s\n",
      "Wall time: 7.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WER 2.967%'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"WER {:.3f}%\".format(dnn.test_net(vgen, sess)[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num2word = [\"ноль\", \"один\", \"два\", \"три\", \"четыре\", \"пять\", \"шесть\", \"семь\", \"восемь\", \"девять\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FtrFile\n",
    "test_mfcc = 'ark,t:../data/test_mfcc.txtftr' # признаки записей \n",
    "resName = 'decode_results_mfcc_ctc'            # файл с результатами декодирования\n",
    "testName = 'test_ref.txt'             # файл с текстом записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 0\n",
    "numbFrame = 0\n",
    "\n",
    "all_args = []\n",
    "for filename, features in FtrFile.FtrDirectoryReader(test_mfcc):\n",
    "    if num_file < 1000:\n",
    "        num_file += 1\n",
    "        numbFrame += features.nSamples\n",
    "        all_args.append(filename)\n",
    "    else: break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dir1 = \"../data/lip_reading/synchronized/test/\"\n",
    "test_dir2 = \"../data/lip_reading/synchronized/video_features/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_decoding(filename):\n",
    "    cur_mfcc_train = np.load(os.path.join(test_dir1, filename + \".npz\"))[\"mfcc\"]\n",
    "    cur_video_features = np.load(os.path.join(test_dir2, filename + \".npz\"))[\"video_features\"]\n",
    "    nums = dnn.get_predictions(sess, np.hstack((cur_mfcc_train, cur_video_features)), np.array(cur_mfcc_train.shape[0]).reshape(-1, )).values\n",
    "#     print(nums)\n",
    "    words = \" \".join(map(lambda x: num2word[x], nums))\n",
    "#     print(words)\n",
    "    return filename + \" \" + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.45it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for filename in tqdm.tqdm(all_args):\n",
    "    results.append(run_decoding(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeWer(testName, resName):\n",
    "    \n",
    "    WER = wer.computeWER(testName, resName)\n",
    "    print('\\n' + '-'*10 + 'RESULT OF RECOGNITION:' + '--'*10 + '\\n')\n",
    "    print('%WER is {}'.format(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------RESULT OF RECOGNITION:--------------------\n",
      "\n",
      "%WER is 5.13\n"
     ]
    }
   ],
   "source": [
    "with open(resName, 'w') as fn:\n",
    "    for result in results:\n",
    "        fn.write(result + \"\\n\")\n",
    "\n",
    "#4. Compute WER:\n",
    "computeWer(testName, resName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml3gpu)",
   "language": "python",
   "name": "ml3gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
