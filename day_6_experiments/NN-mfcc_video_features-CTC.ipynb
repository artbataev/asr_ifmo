{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_files.pickle\", \"rb\") as f:\n",
    "    files_train = pickle.load(f)\n",
    "with open(\"val_files.pickle\", \"rb\") as f:\n",
    "    files_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8433/8433 [00:32<00:00, 256.06it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_train = []\n",
    "phonemes_train = []\n",
    "mfcc_train = []\n",
    "fbanks_train = []\n",
    "video_features_train = []\n",
    "# video_train = []\n",
    "for file in tqdm.tqdm(files_train):\n",
    "    with np.load(file) as data:\n",
    "        cur_labels = data[\"labels\"]\n",
    "        cur_phonemes = data[\"phonemes\"]\n",
    "        cur_mfcc = data[\"mfcc\"]\n",
    "        cur_mfcc = (cur_mfcc - cur_mfcc.mean(axis=0))/cur_mfcc.std(axis=0)\n",
    "        cur_fbanks = data[\"fbanks\"]\n",
    "        cur_fbanks = (cur_fbanks - cur_fbanks.mean(axis=0)) / cur_fbanks.std(axis=0)\n",
    "    #     cur_video = np.load(file)[\"video\"]\n",
    "    labels_train.append(cur_labels)\n",
    "    phonemes_train.append(cur_phonemes)\n",
    "    mfcc_train.append(cur_mfcc)\n",
    "    fbanks_train.append(cur_fbanks)\n",
    "    \n",
    "    with np.load(os.path.join(\"../data/lip_reading/synchronized/video_features/train/\", os.path.basename(file))) as data2:\n",
    "        cur_video_features = data2[\"video_features\"]\n",
    "    video_features_train.append(cur_video_features)\n",
    "    #     video_train.append(cur_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:03<00:00, 301.63it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_val = []\n",
    "phonemes_val = []\n",
    "mfcc_val = []\n",
    "video_val = []\n",
    "video_features_val = []\n",
    "fbanks_val = []\n",
    "for file in tqdm.tqdm(files_val):\n",
    "    with np.load(file) as data:\n",
    "        cur_labels = data[\"labels\"]\n",
    "        cur_phonemes = data[\"phonemes\"]\n",
    "        cur_mfcc = data[\"mfcc\"]\n",
    "        cur_mfcc = (cur_mfcc - cur_mfcc.mean(axis=0))/cur_mfcc.std(axis=0)\n",
    "        cur_fbanks = data[\"fbanks\"]\n",
    "        cur_fbanks = (cur_fbanks - cur_fbanks.mean(axis=0)) / cur_fbanks.std(axis=0)\n",
    "    labels_val.append(cur_labels)\n",
    "    phonemes_val.append(cur_phonemes)\n",
    "    mfcc_val.append(cur_mfcc)\n",
    "    fbanks_val.append(cur_fbanks)\n",
    "    \n",
    "    with np.load(os.path.join(\"../data/lip_reading/synchronized/video_features/train/\", os.path.basename(file))) as data2:\n",
    "        cur_video_features = data2[\"video_features\"]\n",
    "    video_features_val.append(cur_video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, filter_width, num_filters, strides=1, use_relu=True, use_batchnorn=True, is_training=None):\n",
    "    convolution_out = tf.layers.conv1d(prev_layer, num_filters, filter_width, strides=strides, padding=\"same\", \n",
    "                                  activation=None)\n",
    "    if use_batchnorn:\n",
    "        if is_training is None:\n",
    "            raise Exception(\"is_training placeholder required\")\n",
    "        convolution_out = tf.layers.batch_normalization(convolution_out, training=is_training)\n",
    "    if use_relu:\n",
    "        convolution_out = tf.nn.relu(convolution_out)\n",
    "    return convolution_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcFullDNNModel:\n",
    "    def __init__(self, num_features=13, num_symbols=23+1, optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_mfcc\"):\n",
    "        tf.reset_default_graph()\n",
    "        self.num_features = num_features\n",
    "        self.num_symbols = num_symbols\n",
    "        self.checkpoints_folder = folder\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.min_dev_loss = np.float('inf')\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.optimizer_type = optimizer\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self._build_graph()\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        self.features_placeholder = tf.placeholder(tf.float32, [None, None, self.num_features + 512], name=\"features\")\n",
    "        self.features_len_placeholder = tf.placeholder(tf.int32, [None], name=\"features_len\")\n",
    "        self.target_placeholder = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "        self.target_len_placeholder = tf.placeholder(tf.int32, [None], name=\"targets_len\")\n",
    "\n",
    "        self.learning_rate = tf.placeholder_with_default(1e-4, [], name=\"learning_rate\")\n",
    "        self.is_training = tf.placeholder_with_default(False, [], name=\"is_training\")\n",
    "\n",
    "        # transform target to sparse\n",
    "        target_ind = tf.where(tf.not_equal(tf.sequence_mask(self.target_len_placeholder), False))\n",
    "        target_val = tf.gather_nd(self.target_placeholder, target_ind)\n",
    "        self.target_sparse = tf.SparseTensor(target_ind, target_val,\n",
    "                                             tf.cast(tf.shape(self.target_placeholder), dtype=tf.int64))\n",
    "\n",
    "    def _inference(self):\n",
    "        # define architecture\n",
    "        outputs = self.features_placeholder\n",
    "        if self.use_batchnorm:\n",
    "            outputs = tf.layers.batch_normalization(outputs, training=self.is_training)\n",
    "        outputs = conv_layer(outputs, filter_width=48, strides=2, num_filters=256,\n",
    "                             use_batchnorn=self.use_batchnorm,\n",
    "                             is_training=self.is_training)\n",
    "\n",
    "        for layer_id in range(7):\n",
    "            outputs = conv_layer(outputs, filter_width=7, num_filters=256,\n",
    "                                 use_batchnorn=self.use_batchnorm,\n",
    "                                 is_training=self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, filter_width=32, strides=2, num_filters = 256 * 4,\n",
    "                    use_batchnorn = self.use_batchnorm,\n",
    "                    is_training = self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, filter_width=1, strides=2, num_filters=256, \n",
    "                use_batchnorn = self.use_batchnorm,\n",
    "                is_training = self.is_training)\n",
    "\n",
    "        outputs = conv_layer(outputs, 1, num_filters=self.num_symbols, use_relu=False,\n",
    "        use_batchnorn = False)\n",
    "\n",
    "        logits_batch_major = outputs\n",
    "\n",
    "        self.logits = logits_batch_major  # tf.transpose(logits_batch_major, [1, 0, 2])\n",
    "        self.logits_time_major = tf.transpose(logits_batch_major, [1, 0, 2])\n",
    "\n",
    "\n",
    "    def _create_loss(self):\n",
    "        self.predictions, _ = tf.nn.ctc_greedy_decoder(self.logits_time_major,\n",
    "                                                       self.features_len_placeholder // 8)  # predictions - sparse tensor!\n",
    "        self.predictions, _ = tf.nn.ctc_beam_search_decoder(self.logits_time_major,\n",
    "                                                       self.features_len_placeholder // 8, beam_width=200)\n",
    "        self.LER = tf.edit_distance(self.predictions[0], tf.cast(self.target_sparse, tf.int64))\n",
    "        with tf.name_scope('training'):\n",
    "            cost = tf.nn.ctc_loss(self.target_sparse, self.logits_time_major, self.features_len_placeholder // 8, time_major=True)\n",
    "            corrected_cost = tf.minimum(cost, 400.0)\n",
    "            self.loss = tf.reduce_mean(corrected_cost, name='average_loss')\n",
    "\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        if self.optimizer_type == \"adam\":\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        elif self.optimizer_type == \"yellowfin\":\n",
    "            raise ValueError(\"not now\")\n",
    "        else:\n",
    "            raise ValueError(\"incorrect optimizer\")\n",
    "\n",
    "        gvs = self.optimizer.compute_gradients(self.loss)\n",
    "        gradients, trainables = zip(*gvs)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients, 5.0, name='clip_gradients')\n",
    "        # batch normalization in tensorflow requires this extra dependency\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, trainables),\n",
    "                                                             global_step=self.global_step)\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self._create_placeholders()\n",
    "        self._inference()\n",
    "        self._create_loss()\n",
    "        self._network_vars = tf.global_variables()  # without optimizer\n",
    "        self._create_optimizer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        # self._create_summary()\n",
    "\n",
    "\n",
    "    def save_weights(self, sess):\n",
    "        self.saver.save(sess, \n",
    "            '/home/artbataev/Documents/checkpoints/{}/adam_checkpoints_best/ckpt'.format(self.checkpoints_folder), \n",
    "            self.step)\n",
    "\n",
    "\n",
    "    def restore_weights(self, sess, only_network=False):\n",
    "        ckpt = tf.train.get_checkpoint_state(\n",
    "            '/home/artbataev/Documents/checkpoints/{}/adam_checkpoints_best/'.format(self.checkpoints_folder))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            if only_network:\n",
    "                saver = tf.train.Saver(self._network_vars)\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "    def init_op(self, sess, restore=True, restore_only_network=False):\n",
    "        if restore:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.restore_weights(sess, only_network=restore_only_network)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     def get_predictions_p(self, features, features_len):\n",
    "#         feed_dict = {\n",
    "#                 self.features_placeholder: features.reshape(1, features.shape[0], features.shape[1]),\n",
    "#                 self.features_len_placeholder: features_len}\n",
    "#         pred_p = sess.run(self.predictions_prob, feed_dict=feed_dict)\n",
    "#         return pred_p[0]\n",
    "    \n",
    "    def get_predictions(self, sess, features, features_len):\n",
    "        feed_dict = {\n",
    "                self.features_placeholder: features[np.newaxis,...],\n",
    "                self.features_len_placeholder: features_len,\n",
    "            }\n",
    "        pred = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "        return pred[0]\n",
    "    \n",
    "    def test_net(self, batch_generator, sess, verbose=False):\n",
    "        last = False\n",
    "        full_len = 0\n",
    "        loss = 0.0\n",
    "        LER = 0.0\n",
    "        while not last:\n",
    "            texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "            feed_dict = {\n",
    "                self.features_placeholder: mfcc,\n",
    "                self.features_len_placeholder: mfcc_len,\n",
    "                self.target_placeholder: texts,\n",
    "                self.target_len_placeholder: texts_len\n",
    "            }\n",
    "\n",
    "            current_loss, current_LER = sess.run([self.loss, self.LER], feed_dict=feed_dict)\n",
    "\n",
    "            loss += current_loss * len(texts)\n",
    "            LER += np.sum(current_LER)\n",
    "            full_len += len(texts)\n",
    "        \n",
    "        LER /= full_len\n",
    "        loss /= full_len\n",
    "        return loss, LER\n",
    "\n",
    "    def run_step(self, batch_generator, sess, lr=1e-4):\n",
    "        texts, texts_len, mfcc, mfcc_len, last = next(batch_generator)\n",
    "        feed_dict = {\n",
    "            self.features_placeholder: mfcc,\n",
    "            self.features_len_placeholder: mfcc_len,\n",
    "            self.target_placeholder: texts,\n",
    "            self.target_len_placeholder: texts_len,\n",
    "            self.is_training: True,\n",
    "            self.learning_rate: lr,\n",
    "        }\n",
    "        current_loss, current_LER, _ = sess.run([self.loss, self.LER, self.train_step], feed_dict=feed_dict)\n",
    "        self.step += 1\n",
    "        if last:\n",
    "            self.epoch += 1\n",
    "            print(\"Epoch {}, end of dataset\".format(self.epoch))\n",
    "        return current_loss, current_LER\n",
    "\n",
    "    def fit(self, batch_generator, dev_generator, sess, steps=1, lr=1e-4):\n",
    "        try:\n",
    "            train_loss = 0.0\n",
    "            train_LER = 0.0\n",
    "            for _ in range(steps):\n",
    "                current_loss, current_LER = self.run_step(batch_generator, sess, lr)\n",
    "                train_loss += current_loss\n",
    "                train_LER += np.mean(current_LER)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss /= 10\n",
    "                    train_LER /= 10\n",
    "                    print(\"Epoch {} step {} average_loss: {:.5f} LER: {:.2f}%\".format(\n",
    "                        self.epoch, self.step, train_loss, train_LER * 100))\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "\n",
    "                if self.step % 20 == 0 and train_loss < 350:\n",
    "                    epoch_test_loss, epoch_LER = self.test_net(dev_generator, sess)\n",
    "                    improved_text = \"improved\" if epoch_test_loss < self.min_dev_loss else \"not improved\"\n",
    "                    print(\"Step {} test_loss: {:.5f} LER: {:.2f}%\".format(self.step, epoch_test_loss,\n",
    "                                                                          np.mean(epoch_LER) * 100), improved_text)\n",
    "                    if epoch_test_loss < self.min_dev_loss:\n",
    "                        self.min_dev_loss = epoch_test_loss\n",
    "                        self.save_weights(sess)\n",
    "\n",
    "                if self.step % 10 == 0:\n",
    "                    train_loss = 0.0\n",
    "                    train_LER = 0.0\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        all_indices = np.random.choice(len(labels_train), len(labels_train), replace=False)\n",
    "        for j in range(0, len(labels_train), batch_size):\n",
    "            indices = all_indices[j: j + batch_size]\n",
    "            texts_len = np.array(list(map(lambda x: len(x), (labels_train[i] for i in indices))))\n",
    "            mfcc_len = np.array(list(map(lambda x: len(x), (mfcc_train[i] for i in indices))))\n",
    "            texts = np.zeros((texts_len.shape[0], np.max(texts_len)), dtype=np.int)\n",
    "            features = np.zeros((texts_len.shape[0], np.max(mfcc_len), 13 + 512))\n",
    "            #         texts, texts_len, mfcc, mfcc_len, last\n",
    "            for i in range(texts_len.shape[0]):\n",
    "                texts[i, :len(labels_train[indices[i]])] = labels_train[indices[i]]\n",
    "                features[i, :len(mfcc_train[indices[i]]), :] = np.hstack((mfcc_train[indices[i]], video_features_train[indices[i]]))\n",
    "            yield texts, texts_len, features, mfcc_len, j + batch_size >= len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_generator():\n",
    "    global batch_size\n",
    "    while True:\n",
    "        for j in range(0, len(labels_val), batch_size):\n",
    "            indices = np.arange(j, min(j + batch_size, len(labels_val)))\n",
    "            texts_len = np.array(list(map(lambda x: len(x), (labels_val[i] for i in indices))))\n",
    "            mfcc_len = np.array(list(map(lambda x: len(x), (mfcc_val[i] for i in indices))))\n",
    "            texts = np.zeros((texts_len.shape[0], np.max(texts_len)), dtype=np.int)\n",
    "            features = np.zeros((texts_len.shape[0], np.max(mfcc_len), 13 + 512))\n",
    "            #         texts, texts_len, mfcc, mfcc_len, last\n",
    "            for i in range(texts_len.shape[0]):\n",
    "                texts[i, :len(labels_val[indices[i]])] = labels_val[indices[i]]\n",
    "                features[i, :len(mfcc_val[indices[i]]), :] = np.hstack((mfcc_val[indices[i]], video_features_val[indices[i]]))\n",
    "            yield texts, texts_len, features, mfcc_len, j + batch_size >= len(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vgen = val_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tgen = train_generator()\n",
    "vgen = val_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts, texts_len, mfcc, mfcc_len, last = next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn = CtcFullDNNModel(optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_full_ctc\")\n",
    "sess = tf.Session()\n",
    "dnn.init_op(sess, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Epoch 0 step 10 average_loss: 44.81107 LER: 142.23%\n",
      ".........Epoch 0 step 20 average_loss: 6.63214 LER: 23.31%\n",
      "Step 20 test_loss: 80.45757 LER: 201.37% improved\n",
      ".........Epoch 0 step 30 average_loss: 2.41875 LER: 7.22%\n",
      "..Epoch 1, end of dataset\n",
      ".......Epoch 1 step 40 average_loss: 1.87972 LER: 5.52%\n",
      "Step 40 test_loss: 45.39858 LER: 49.34% improved\n",
      ".........Epoch 1 step 50 average_loss: 1.43005 LER: 3.99%\n",
      ".........Epoch 1 step 60 average_loss: 1.39606 LER: 4.35%\n",
      "Step 60 test_loss: 21.55608 LER: 9.11% improved\n",
      ".....Epoch 2, end of dataset\n",
      "....Epoch 2 step 70 average_loss: 1.00453 LER: 3.02%\n",
      ".........Epoch 2 step 80 average_loss: 0.84314 LER: 2.70%\n",
      "Step 80 test_loss: 12.60497 LER: 20.12% improved\n",
      ".........Epoch 2 step 90 average_loss: 0.80382 LER: 2.60%\n",
      "........Epoch 3, end of dataset\n",
      ".Epoch 3 step 100 average_loss: 0.78834 LER: 2.53%\n",
      "Step 100 test_loss: 6.80537 LER: 24.93% improved\n",
      ".........Epoch 3 step 110 average_loss: 0.56695 LER: 1.83%\n",
      ".........Epoch 3 step 120 average_loss: 0.60539 LER: 2.04%\n",
      "Step 120 test_loss: 4.99567 LER: 13.63% improved\n",
      ".........Epoch 3 step 130 average_loss: 0.52282 LER: 1.79%\n",
      ".Epoch 4, end of dataset\n",
      "........Epoch 4 step 140 average_loss: 0.44515 LER: 1.39%\n",
      "Step 140 test_loss: 3.44288 LER: 9.77% improved\n",
      ".........Epoch 4 step 150 average_loss: 0.38138 LER: 1.14%\n",
      ".........Epoch 4 step 160 average_loss: 0.41697 LER: 1.38%\n",
      "Step 160 test_loss: 3.52619 LER: 14.04% not improved\n",
      "....Epoch 5, end of dataset\n",
      ".....Epoch 5 step 170 average_loss: 0.30197 LER: 1.02%\n",
      ".........Epoch 5 step 180 average_loss: 0.31267 LER: 0.98%\n",
      "Step 180 test_loss: 1.98267 LER: 4.89% improved\n",
      ".........Epoch 5 step 190 average_loss: 0.24453 LER: 0.82%\n",
      ".......Epoch 6, end of dataset\n",
      "..Epoch 6 step 200 average_loss: 0.35835 LER: 1.24%\n",
      "Step 200 test_loss: 2.16727 LER: 6.96% not improved\n",
      ".........Epoch 6 step 210 average_loss: 0.24368 LER: 0.78%\n",
      ".........Epoch 6 step 220 average_loss: 0.24128 LER: 0.84%\n",
      "Step 220 test_loss: 1.59588 LER: 4.95% improved\n",
      ".........Epoch 6 step 230 average_loss: 0.23980 LER: 0.77%\n",
      "Epoch 7, end of dataset\n",
      ".........Epoch 7 step 240 average_loss: 0.24297 LER: 0.78%\n",
      "Step 240 test_loss: 1.40181 LER: 3.88% improved\n",
      ".........Epoch 7 step 250 average_loss: 0.17590 LER: 0.70%\n",
      ".........Epoch 7 step 260 average_loss: 0.18704 LER: 0.62%\n",
      "Step 260 test_loss: 1.19967 LER: 3.36% improved\n",
      "...Epoch 8, end of dataset\n",
      "......Epoch 8 step 270 average_loss: 0.18414 LER: 0.56%\n",
      ".........Epoch 8 step 280 average_loss: 0.13406 LER: 0.39%\n",
      "Step 280 test_loss: 1.32331 LER: 3.39% not improved\n",
      ".........Epoch 8 step 290 average_loss: 0.14441 LER: 0.52%\n",
      "......Epoch 9, end of dataset\n",
      "...Epoch 9 step 300 average_loss: 0.13383 LER: 0.46%\n",
      "Step 300 test_loss: 1.39971 LER: 3.69% not improved\n",
      ".........Epoch 9 step 310 average_loss: 0.15819 LER: 0.57%\n",
      ".........Epoch 9 step 320 average_loss: 0.13031 LER: 0.49%\n",
      "Step 320 test_loss: 1.36000 LER: 3.39% not improved\n",
      ".........Epoch 10, end of dataset\n",
      "Epoch 10 step 330 average_loss: 0.15390 LER: 0.49%\n",
      ".........Epoch 10 step 340 average_loss: 0.11794 LER: 0.45%\n",
      "Step 340 test_loss: 1.23592 LER: 3.44% not improved\n",
      ".........Epoch 10 step 350 average_loss: 0.10382 LER: 0.42%\n",
      ".........Epoch 10 step 360 average_loss: 0.13789 LER: 0.43%\n",
      "Step 360 test_loss: 1.44189 LER: 3.22% not improved\n",
      "..Epoch 11, end of dataset\n",
      "..Training Interrupted\n"
     ]
    }
   ],
   "source": [
    "dnn.fit(tgen, vgen, sess, steps=1000, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/artbataev/Documents/checkpoints/dnn_full_ctc_best/adam_checkpoints_best/ckpt-260\n"
     ]
    }
   ],
   "source": [
    "dnn = CtcFullDNNModel(optimizer=\"adam\", use_batchnorm=True, folder=\"dnn_full_ctc_best\")\n",
    "sess = tf.Session()\n",
    "dnn.init_op(sess, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 s, sys: 864 ms, total: 6.51 s\n",
      "Wall time: 7.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WER 2.967%'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"WER {:.3f}%\".format(dnn.test_net(vgen, sess)[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num2word = [\"ноль\", \"один\", \"два\", \"три\", \"четыре\", \"пять\", \"шесть\", \"семь\", \"восемь\", \"девять\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FtrFile\n",
    "test_mfcc = 'ark,t:../data/test_mfcc.txtftr' # признаки записей \n",
    "resName = 'decode_resultss'            # файл с результатами декодирования\n",
    "testName = 'test_ref.txt'             # файл с текстом записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 0\n",
    "numbFrame = 0\n",
    "\n",
    "all_args = []\n",
    "for filename, features in FtrFile.FtrDirectoryReader(test_mfcc):\n",
    "    if num_file < 1000:\n",
    "        num_file += 1\n",
    "        numbFrame += features.nSamples\n",
    "        all_args.append(filename)\n",
    "    else: break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dir1 = \"../data/lip_reading/synchronized/test/\"\n",
    "test_dir2 = \"../data/lip_reading/synchronized/video_features/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_decoding(filename):\n",
    "    cur_mfcc_train = np.load(os.path.join(test_dir1, filename + \".npz\"))[\"mfcc\"]\n",
    "    cur_video_features = np.load(os.path.join(test_dir2, filename + \".npz\"))[\"video_features\"]\n",
    "    nums = dnn.get_predictions(sess, np.hstack((cur_mfcc_train, cur_video_features)), np.array(cur_mfcc_train.shape[0]).reshape(-1, )).values\n",
    "#     print(nums)\n",
    "    words = \" \".join(map(lambda x: num2word[x], nums))\n",
    "#     print(words)\n",
    "    return filename + \" \" + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 56.37it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for filename in tqdm.tqdm(all_args):\n",
    "    results.append(run_decoding(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeWer(testName, resName):\n",
    "    \n",
    "    WER = wer.computeWER(testName, resName)\n",
    "    print('\\n' + '-'*10 + 'RESULT OF RECOGNITION:' + '--'*10 + '\\n')\n",
    "    print('%WER is {}'.format(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------RESULT OF RECOGNITION:--------------------\n",
      "\n",
      "%WER is 4.81\n"
     ]
    }
   ],
   "source": [
    "with open(resName, 'w') as fn:\n",
    "    for result in results:\n",
    "        fn.write(result + \"\\n\")\n",
    "\n",
    "#4. Compute WER:\n",
    "computeWer(testName, resName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml3gpu)",
   "language": "python",
   "name": "ml3gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
