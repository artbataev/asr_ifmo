{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 3\n",
    "\n",
    "В данном уроке мы попробуем запустить наше DTW уже на реальном звуке. Для начала это будут простые слова \"да\" и \"нет\". Имеется небольшая база с файлами различных вариаций произнесения этих слов. Часть из них (эталоны) будут использованны для построения графа. Для остальных же (записей) будет поочередно запущен DTW алгорим для определения ближайшего к ним файла из эталонов.\n",
    "\n",
    "MFCC признаки мы уже посчитали и сохранили в формате ark в файлах etalons_mfcc.txtftr и records_mfcc.txtftr соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее, наш граф был способен идти только вровень, либо сжимать запись (оставаться в том же узле графа) относительно кадров эталона. Но необходимо еще уметь и растягивать запись. Для этого нужно ввести дополнительные переходы через один и два состояния для узлов графа.\n",
    "\n",
    "<br>\n",
    "<img src=\"graph.png\">\n",
    "<br>\n",
    "\n",
    "<b>Задание 1:</b>\n",
    "Добавить для узлов графа дополнительные переходы через один и два состояния (нулевой узел должен остаться прежним)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import FtrFile\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, ftr, idx):\n",
    "        self.ftr = ftr           # вектор признаков узла \n",
    "        self.isFinal = False     # является ли этот узел финальнвм в слове\n",
    "        self.word = None         # слово эталона (назначается только для финального узла)       \n",
    "        self.nextStates = []     # список следующих узлов\n",
    "        self.idx = idx           # индекс узла\n",
    "        self.bestToken = None    # лучший токен (по минимуму дистанции) в узле\n",
    "        self.currentWord = None  # текущее слово эталона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph(rxfilename, num_next=3):\n",
    "    assert num_next >= 1\n",
    "    startState = State(None, 0)\n",
    "    graph = [startState, ]\n",
    "    stateIdx = 1\n",
    "    for word, features in FtrFile.FtrDirectoryReader(rxfilename):\n",
    "        prevStates = [startState, ]\n",
    "        for frame in range(features.nSamples):\n",
    "            state = State(features.readvec(), stateIdx)\n",
    "            state.currentWord = word           # слово эталона теперь будет храниться в каждом узле\n",
    "            state.nextStates.append(state)  \n",
    "            for prevState in prevStates:\n",
    "                prevState.nextStates.append(state) \n",
    "            if prevStates[0] == startState: # only root\n",
    "                prevStates = [state,]\n",
    "            else: \n",
    "                prevStates.append(state)\n",
    "                prevStates = prevStates[-num_next:]\n",
    "            graph.append(state)\n",
    "            stateIdx += 1\n",
    "            \n",
    "        if state:\n",
    "            state.word = word\n",
    "            state.isFinal = True\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий блок кода проверяет ваш граф на некоторые ключевые параметры и записывает в удобном для чтения виде в файл graph.txt. Сравните его с заведомо правильным графом, сохраненным в graph_reference.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SEE graph.txt ***\n",
      "*** END DEBUG. GRAPH ***\n"
     ]
    }
   ],
   "source": [
    "def check_graph(graph):\n",
    "    assert len(graph) > 0, \"graph is empty.\"\n",
    "    assert graph[0].ftr is None \\\n",
    "        and graph[0].word is None \\\n",
    "        and not graph[0].isFinal, \"broken start state in graph.\"\n",
    "    idx = 0\n",
    "    for state in graph:\n",
    "        assert state.idx == idx\n",
    "        idx += 1\n",
    "        assert (state.isFinal and state.word is not None) \\\n",
    "            or (not state.isFinal and state.word is None)\n",
    "\n",
    "\n",
    "def print_graph(graph):\n",
    "    with open('graph.txt', 'w') as fn:\n",
    "        np.set_printoptions(formatter={'float': '{: 0.1f}'.format})\n",
    "        for state in graph:\n",
    "            nextStatesIdxs = [s.idx for s in state.nextStates]\n",
    "            fn.write(\"State: idx={} word={} isFinal={} nextStatesIdxs={} ftr={} \\n\".format(\n",
    "                state.idx, state.word, state.isFinal, nextStatesIdxs, state.ftr))\n",
    "    print(\"*** SEE graph.txt ***\")\n",
    "    print(\"*** END DEBUG. GRAPH ***\")\n",
    "\n",
    "    \n",
    "etalons = \"ark,t:etalons_mfcc.txtftr\"\n",
    "graph = load_graph(etalons)\n",
    "check_graph(graph)\n",
    "# Сохранить граф в читабельном виде в файл graph.txt:\n",
    "print_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализованный в прошлом уроке TPA в данном случае будет перебирать все возможные варианты разметки, что приведет к значительному увеличению времени работы нашего DTW. Для решения этой проблемы мы будем отбрасывать \"ненужные\" токены еще на этапе прохождения по графу. Этим занимаются, так называемые, beam и state prunings.\n",
    "\n",
    "### state pruning:\n",
    "В классе State нужно добавить атрибут best_token – ссылку на лучший токен, заканчивающийся в данном стейте на данном кадре записи. После порождения всех токенов за текущий кадр записи, пройдемся по каждому из полученных nextTokens, затем впишем текущий токен в State.best_token (здесь State – это узел, на котором закончился токен), убив предыдущий лучший токен, либо убьем сам токен, если он хуже лучшего на этом узле. За жизнеспособность токена отвечает его атрибут is_alive: True или False соответственно.\n",
    "\n",
    "После этого необходимо очистить поле best_token у всех узлов графа.\n",
    "\n",
    "### beam pruning:\n",
    "Идея состоит в том, чтобы на каждом кадре записи находить плохие токены и откидывать их (token.is_alive = False). \n",
    "Плохие – это,очевидно, накопившие слишком большое отклонение от стейтов,по которым они идут. Слишком большое отклонение – это непонятно какое (может токен плохой, может слово слишком длинное, может звук очень плохой – не разобрать).\n",
    "\n",
    "Поэтому плохость токена считают относительно лучшего токена. Заведем переменную thr_common (обычно её называют beam – ширина луча поиска; у нас это common threshold – “общий порог” – по историческим причинам). И если token.dist > best_token.dist + thr_common, то token плохой и мы его отбросим.\n",
    "\n",
    "Выкидывая какой-то токен из-за его отклонеиня, мы рискуем тем, что через сколько-то кадров все потомки выживших токенов могут оказаться очень плохими, а только потомки отброшенного токена оказались бы чудо как хороши. То есть, вводя thr_common, мы вводим ошибку.\n",
    "Поэтому thr_common нужно подобрать так, чтобы скорость сильно выросла, а ошибка выросла незначительно.\n",
    "\n",
    "<br>\n",
    "Введение этих методов может привести к тому, что у нас просто не окажется в конце выживших токенов в финальных узлах графа. Для того, чтобы иметь возможность выдавать результат в этом случае, мы введем дополнительный атрибут currentWord у класса State. Теперь в любом узле каждой ветви будет храниться слово соответствующего эталона для этой ветви. \n",
    "\n",
    "Тогда в конце работы DTW, если у нас не будет живых финальных токенов, то мы просто выберем лучший из оставшихся и по полю state.currentWord определим слово эталона.\n",
    "\n",
    "<b>Задание 2:</b> \n",
    "- Написать функцию findBest для поиска токена с минимальной дистанцией.\n",
    "- Реализовать функции для state и beam pruning (здесь нам и может пригодиться функция findBest).\n",
    "- Разобраться с вычислением WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, state, dist=0.0, sentence=\"\"):\n",
    "        self.state = state\n",
    "        self.dist = dist\n",
    "        self.sentence = sentence\n",
    "        self.alive = True\n",
    "    \n",
    "\n",
    "def findBest(Tokens):\n",
    "    #---------------------------------------TODO---------------------------------  \n",
    "    bestToken = Tokens[np.argmin([token.dist for token in Tokens])]\n",
    "    #-----------------------------------------------------------------------------\n",
    "    return bestToken\n",
    "\n",
    "\n",
    "def beamPruning(nextTokens, thr_common=70):\n",
    "#     thr_common = 70 # можно менять\n",
    "    #--------------------------------TODO--------------------------------------\n",
    "    # 1. Ищем лучший токен из nextTokens с помощью findBest\n",
    "    # 2. Присваиваем token.aliv значение False, если дистанция этого токена больше, чем\n",
    "    #    длина лучшего токена + thr_common\n",
    "    \n",
    "    bestToken = findBest(nextTokens)\n",
    "    bestDist = bestToken.dist\n",
    "    for token in nextTokens:\n",
    "        if token.alive and token.dist > bestDist + thr_common:\n",
    "            token.alive = False\n",
    "    #--------------------------------------------------------------------------\n",
    "    return [token for token in nextTokens if token.alive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statePruning(nextTokens):\n",
    "    for token in nextTokens:\n",
    "        #--------------------------TODO---------------------------------\n",
    "        if token.state.bestToken is None:\n",
    "            token.state.bestToken = token\n",
    "        elif token.state.bestToken.dist > token.dist:\n",
    "            token.state.bestToken.alive = False\n",
    "            token.state.bestToken = token\n",
    "        else: # token.state.bestToken.dist < token.dist\n",
    "            token.alive = False\n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "    # Сбросить bestToken на None для всеx узлов графа\n",
    "    for token in nextTokens:\n",
    "        token.state.bestToken = None\n",
    "    #-------------------------------------------------------------------\n",
    "    return [token for token in nextTokens if token.alive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(X, Y):\n",
    "#     result = float(np.sqrt(sum(pow(X - Y, 2))))\n",
    "    result = np.linalg.norm(X - Y)\n",
    "#     result = 1 - np.sum(X * Y) / (np.linalg.norm(X) * np.linalg.norm(Y))\n",
    "#     result = np.sum(np.abs(X - Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(array, l, r):\n",
    "    elem = array[(l + r) // 2]\n",
    "    left_arr = [array[i] for i in range(l, r + 1) if array[i] < elem]\n",
    "    mid_arr = [array[i] for i in range(l, r + 1) if array[i] == elem]\n",
    "    right_arr = [array[i] for i in range(l, r + 1) if array[i] > elem]\n",
    "    array[l: r+1] = left_arr + mid_arr + right_arr\n",
    "    return l + len(left_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_k_statistics(array, k):\n",
    "#     assert len(array) >= k\n",
    "    k -= 1\n",
    "    left = 0\n",
    "    right = len(array) - 1\n",
    "    while(True):\n",
    "        mid = partition(array, left, right)\n",
    "        if mid == k:\n",
    "            return array[mid]\n",
    "        elif k < mid:\n",
    "            right = mid\n",
    "        else: # k > mid\n",
    "            left = mid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.0 1.0 2.0, step=1.0, n_buckets = 1\n",
    "# 2.0 // 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_k_statistics_bucket(array, k):\n",
    "    assert 1 <= k <= len(array)\n",
    "    current_array = array\n",
    "    n_buckets = 10\n",
    "    while True:\n",
    "        min_arg = min(current_array)\n",
    "        max_arg = max(current_array)\n",
    "        if min_arg == max_arg:\n",
    "            return min_arg\n",
    "        step = (max_arg - min_arg) / n_buckets\n",
    "        buckets = [[] for _ in range(n_buckets)]\n",
    "        for elem in current_array:\n",
    "            i = int((elem - min_arg) // step) if elem < max_arg else n_buckets - 1\n",
    "            buckets[i].append(elem)\n",
    "        i = 0\n",
    "        while len(buckets[i]) < k:\n",
    "            k -= len(buckets[i])\n",
    "            i += 1\n",
    "        current_array = buckets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arr = np.random.randint(355, size=(10,)).tolist()\n",
    "# print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_best_k(nextTokens, k=100):\n",
    "#     assert len(nextTokens) > k\n",
    "    distances = [token.dist for token in nextTokens]\n",
    "    k_elem = find_k_statistics_bucket(distances, k)\n",
    "    \n",
    "    filteredTokens = [token for token in nextTokens if token.dist < k_elem]\n",
    "    addTokens = [token for token in nextTokens if token.dist == k_elem]\n",
    "    result = filteredTokens + addTokens[: k - len(filteredTokens)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize(features, graph, rec_results, thr_common=70, best_k=None):\n",
    "\n",
    "    print(\"-\" * 23)\n",
    "    startTime = time.time()\n",
    "    startState = graph[0]\n",
    "    activeTokens = [Token(startState), ]\n",
    "    nextTokens = []\n",
    "\n",
    "    for frame in range(features.nSamples):\n",
    "        ftrCurrentFrameRecord = features.readvec()\n",
    "        bestNextDist = np.inf\n",
    "        for token in filter(lambda token: token.alive, activeTokens):\n",
    "            for i, transitionState in enumerate(token.state.nextStates):\n",
    "                newDist = token.dist + distance(ftrCurrentFrameRecord, transitionState.ftr)\n",
    "                if bestNextDist + thr_common >= newDist:\n",
    "                    if newDist < bestNextDist:\n",
    "                        bestNextDist = newDist\n",
    "                        \n",
    "                    if transitionState.bestToken is None:\n",
    "                        newToken = Token(transitionState, newDist, token.sentence)\n",
    "                        nextTokens.append(newToken)\n",
    "                        transitionState.bestToken = newToken\n",
    "                    elif transitionState.bestToken.dist > newDist:\n",
    "                        newToken = Token(transitionState, newDist, token.sentence)\n",
    "                        nextTokens.append(newToken)\n",
    "                        transitionState.bestToken.alive = False\n",
    "                        transitionState.bestToken = newToken\n",
    "                    else:\n",
    "                        pass\n",
    "        for token in nextTokens:\n",
    "            token.state.bestToken = None           \n",
    "                    \n",
    "        # state and beam prunings:\n",
    "#         nextTokens = statePruning(nextTokens)         \n",
    "#         nextTokens = beamPruning(nextTokens, thr_common=thr_common) \n",
    "        nextTokens = [token for token in nextTokens if  token.alive and token.dist <= bestNextDist + thr_common]\n",
    "        if best_k is not None and len(nextTokens) > best_k:\n",
    "            nextTokens = filter_best_k(nextTokens, best_k)\n",
    "        \n",
    "        activeTokens = nextTokens\n",
    "        nextTokens = []                                    \n",
    "        \n",
    "    # поиск финальных токенов:\n",
    "    finalTokens = []\n",
    "    for token in activeTokens:\n",
    "        if token.state.isFinal and token.alive:\n",
    "            finalTokens.append(token)\n",
    "\n",
    "    # если нет финальных, то берем лучший из выживших:\n",
    "    if len(finalTokens) != 0:\n",
    "        winToken = findBest(finalTokens)\n",
    "    else:\n",
    "        winToken = findBest(activeTokens)\n",
    "        winToken.state.word = winToken.state.currentWord\n",
    "\n",
    "    # вывод результата DTW\n",
    "    print(\"result: {} ==> {}\".format(filename, winToken.state.word))\n",
    "    endTime = time.time()\n",
    "    print(\"time: {} sec\".format(round(endTime-startTime, 2)))\n",
    "\n",
    "    # совпадает ли запись с полученным эталоном:  \n",
    "    record_word = filename.split('_')[0]\n",
    "    etalon_word = winToken.state.word.split('_')[0]\n",
    "    rec_results.append(etalon_word == record_word)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим нашу программу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "result: da_06 ==> da_02\n",
      "time: 2.3 sec\n",
      "-----------------------\n",
      "result: da_07 ==> da_02\n",
      "time: 4.74 sec\n",
      "-----------------------\n",
      "result: da_08 ==> da_04\n",
      "time: 3.44 sec\n",
      "-----------------------\n",
      "result: da_09 ==> da_02\n",
      "time: 3.44 sec\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8590c482ba52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFtrFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFtrDirectoryReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr_common\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mnumbFrame\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-e776e8c78bd2>\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m(features, graph, rec_results, thr_common, best_k)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextStates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mnewToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitionState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                         \u001b[0mnewToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftrCurrentFrameRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransitionState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                         \u001b[0mnextTokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# transform last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d06b18865594>\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     result = float(np.sqrt(sum(pow(X - Y, 2))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     result = 1 - np.sum(X * Y) / (np.linalg.norm(X) * np.linalg.norm(Y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     result = np.sum(np.abs(X - Y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml3gpu/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2167\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2168\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "etalons = \"ark,t:etalons_mfcc.txtftr\"\n",
    "records = \"ark,t:records_mfcc.txtftr\"\n",
    "\n",
    "rec_results = []  # переменная для подсчета точности распознавания\n",
    "\n",
    "s_time = time.time()\n",
    "numbFrame = 0     # счетчик общего количества кадров для расчета RTF\n",
    "\n",
    "graph = load_graph(etalons, num_next=3)\n",
    "\n",
    "for filename, features in FtrFile.FtrDirectoryReader(records):\n",
    "    frame = recognize(features, graph, rec_results, thr_common=70)\n",
    "    numbFrame += frame\n",
    "\n",
    "print(\"-\" * 23)\n",
    "print(\"WER is: {}\".format(round(1 - sum(rec_results)/len(rec_results), 3)))\n",
    "e_time = time.time()\n",
    "time = e_time-s_time\n",
    "minut = int(time/60)\n",
    "second = int(time-minut*60)\n",
    "print(\"Total time: {} min {} sec\".format(minut, second))\n",
    "rtf = round(time/(numbFrame*0.01), 2)\n",
    "print(\"RTF is: {}\".format(rtf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Задание 3:</b> Подбирите значение порога thr_common и количество дополнительных переходов для узлов так, чтобы получить минимально возможно значение WER для данной базы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "result: da_06 ==> da_02\n",
      "time: 2.75 sec\n",
      "-----------------------\n",
      "result: da_07 ==> da_02\n",
      "time: 4.86 sec\n",
      "-----------------------\n",
      "result: da_08 ==> da_04\n",
      "time: 3.29 sec\n",
      "-----------------------\n",
      "result: da_09 ==> da_02\n",
      "time: 3.27 sec\n",
      "-----------------------\n",
      "result: da_10 ==> da_04\n",
      "time: 4.24 sec\n",
      "-----------------------\n",
      "result: da_11 ==> da_02\n",
      "time: 3.9 sec\n",
      "-----------------------\n",
      "result: da_12 ==> da_05\n",
      "time: 2.89 sec\n",
      "-----------------------\n",
      "result: da_13 ==> da_02\n",
      "time: 3.01 sec\n",
      "-----------------------\n",
      "result: da_14 ==> da_02\n",
      "time: 2.73 sec\n",
      "-----------------------\n",
      "result: da_15 ==> da_02\n",
      "time: 4.94 sec\n",
      "-----------------------\n",
      "result: da_16 ==> da_02\n",
      "time: 2.28 sec\n",
      "-----------------------\n",
      "result: da_17 ==> da_05\n",
      "time: 3.82 sec\n",
      "-----------------------\n",
      "result: da_18 ==> da_05\n",
      "time: 2.14 sec\n",
      "-----------------------\n",
      "result: da_19 ==> da_05\n",
      "time: 3.9 sec\n",
      "-----------------------\n",
      "result: net_06 ==> net_02\n",
      "time: 3.13 sec\n",
      "-----------------------\n",
      "result: net_07 ==> net_04\n",
      "time: 3.36 sec\n",
      "-----------------------\n",
      "result: net_08 ==> net_02\n",
      "time: 3.06 sec\n",
      "-----------------------\n",
      "result: net_09 ==> net_05\n",
      "time: 3.52 sec\n",
      "-----------------------\n",
      "result: net_10 ==> net_02\n",
      "time: 2.54 sec\n",
      "-----------------------\n",
      "result: net_11 ==> net_03\n",
      "time: 3.7 sec\n",
      "-----------------------\n",
      "result: net_12 ==> net_04\n",
      "time: 3.34 sec\n",
      "-----------------------\n",
      "result: net_13 ==> net_04\n",
      "time: 3.04 sec\n",
      "-----------------------\n",
      "result: net_14 ==> net_04\n",
      "time: 4.96 sec\n",
      "-----------------------\n",
      "result: net_15 ==> net_04\n",
      "time: 4.14 sec\n",
      "-----------------------\n",
      "result: net_16 ==> net_04\n",
      "time: 4.03 sec\n",
      "-----------------------\n",
      "result: net_17 ==> net_05\n",
      "time: 4.19 sec\n",
      "-----------------------\n",
      "result: net_18 ==> net_05\n",
      "time: 3.2 sec\n",
      "-----------------------\n",
      "result: net_19 ==> net_04\n",
      "time: 3.52 sec\n",
      "-----------------------\n",
      "result: net_20 ==> net_04\n",
      "time: 4.06 sec\n",
      "-----------------------\n",
      "result: net_21 ==> net_04\n",
      "time: 3.88 sec\n",
      "-----------------------\n",
      "WER is: 0.0\n",
      "Total time: 1 min 46 sec\n",
      "RTF is: 1.28\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "etalons = \"ark,t:etalons_mfcc.txtftr\"\n",
    "records = \"ark,t:records_mfcc.txtftr\"\n",
    "\n",
    "rec_results = []  # переменная для подсчета точности распознавания\n",
    "\n",
    "s_time = time.time()\n",
    "numbFrame = 0     # счетчик общего количества кадров для расчета RTF\n",
    "\n",
    "graph = load_graph(etalons, num_next=3)\n",
    "\n",
    "for filename, features in FtrFile.FtrDirectoryReader(records): \n",
    "    frame = recognize(features, graph, rec_results, thr_common=75) #best_k=300\n",
    "    numbFrame += frame\n",
    "\n",
    "print(\"-\" * 23)\n",
    "print(\"WER is: {}\".format(round(1 - sum(rec_results)/len(rec_results), 3)))\n",
    "e_time = time.time()\n",
    "time = e_time-s_time\n",
    "minut = int(time/60)\n",
    "second = int(time-minut*60)\n",
    "print(\"Total time: {} min {} sec\".format(minut, second))\n",
    "rtf = round(time/(numbFrame*0.01), 2)\n",
    "print(\"RTF is: {}\".format(rtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# без best_k, state_pruning в процессе Total time: 1 min 56 sec\n",
    "# без best_k, без state_pruning в процессе Total time: 1 min 54 sec\n",
    "# all prunings :) Total time: 1 min 48 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_next=3, thr_common=150, best_k=1000, euclidean: WER is: 0.0 Total time: 4 min 25 sec\n",
    "# num_next=3, thr_common=150, best_k=200, euclidean: WER is: 0.033 Total time: 1 min 20 sec\n",
    "# num_next=3, thr_common=150, best_k=500, euclidean: WER is: 0.0 Total time: 2 min 48 sec\n",
    "# num_next=3, thr_common=120, best_k=500, euclidean: WER is: 0.0 Total time: 2 min 39 sec\n",
    "# num_next=3, thr_common=100, best_k=500, euclidean: WER is: 0.0 Total time: 2 min 20 sec\n",
    "# num_next=3, thr_common=100, best_k=350, euclidean: WER is: 0.0 Total time: 1 min 49 sec\n",
    "# num_next=3, thr_common=90, best_k=350, euclidean: WER is: 0.0 Total time: 1 min 44 sec\n",
    "# num_next=3, thr_common=90, best_k=300, euclidean: WER is: 0.0 Total time: 1 min 35 sec\n",
    "# num_next=3, thr_common=80, best_k=300, euclidean: WER is: 0.0 Total time: 1 min 28 sec\n",
    "# num_next=3, thr_common=75, best_k=300, euclidean: WER is: 0.0 Total time: 1 min 26 sec\n",
    "\n",
    "# num_next=3, thr_common=80, best_k=250, euclidean: WER is: 0.033 Total time: 1 min 19 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiments\n",
    "# 1 / 150 => 0.033 wer\n",
    "# 1 / 200 => 0.067 wer\n",
    "# 1 / 100 => 0.033 wer\n",
    "# 1 / 80 => 0.0 wer\n",
    "# 2 / 200 => высокий wer\n",
    "# 2 / 50 => высокий wer\n",
    "# 2 / 180 => высокий wer\n",
    "# 3 / 100 => высокий wer?\n",
    "# 5 / 30 => 0.233 wer\n",
    "# 5 / 70 => высокий wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 200 good choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# как сэкономить на создании классов?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml3gpu)",
   "language": "python",
   "name": "ml3gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
